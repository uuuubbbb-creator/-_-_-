{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a93c9be4",
   "metadata": {},
   "source": [
    "# 인프라 데이터 통합 및 PNU 생성\n",
    "\n",
    "다음 셀은 워크스페이스의 `공원녹지`, `교육인프라`, `유통시설` 폴더에 있는 시기별 CSV 파일들을 읽어 통합하고,\n",
    "`HONO_NM`(번지)에서 `본번`/`부번`을 파싱해 `pnu` 및 19자리 `PNU` 컬럼을 생성합니다.\n",
    "PNU 구성: `11500103001`(공통) + 본번(4자리, 왼쪽0채움) + 부번(4자리, 왼쪽0채움)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3faec98",
   "metadata": {},
   "source": [
    "**작업 요약**\n",
    "\n",
    "- **목표:** 매매·전월세·표제부 + 인프라(공원녹지, 교육인프라, 유통시설) 데이터를 통합하여 마스터 테이블 생성.  \n",
    "- **노트북:** `공원녹지 통합.ipynb`에 코드 및 설명 추가(인프라 통합, PNU 생성, 필터링, 마스터 병합 스크립트 포함).\n",
    "\n",
    "**세부 단계(실행된 작업)**\n",
    "\n",
    "- **1. 워크스페이스 점검:**  \n",
    "  - 인프라 폴더: 공원녹지, 교육인프라, 유통시설 내부 CSV 확인.\n",
    "\n",
    "- **2. 인프라 통합 및 PNU 생성:**  \n",
    "  - 각 폴더의 시기별 CSV들을 합쳐 `*_combined.csv` 생성.  \n",
    "  - 번지(`HONO_NM`) 파싱: `bonbun`, `bubun` 추출 → `bonbun4`/`bubun4`로 4자리 0채움 → `PNU` = `11500103001` + `bonbun4` + `bubun4` (19자리).  \n",
    "  - 중복 소문자 `pnu` 컬럼이 있으면 삭제하고 `PNU`만 유지.\n",
    "\n",
    "- **3. 인프라별 결과(파일 및 요약):**  \n",
    "  - 공원녹지_combined.csv — rows=1,199,502, cols=29, unique PNU=30,036  \n",
    "  - 교육인프라_combined.csv — rows=599,751, cols=29, unique PNU=30,036  \n",
    "  - 유통시설_combined.csv — rows=599,751, cols=29, unique PNU=30,036\n",
    "\n",
    "- **4. 특정 동 필터링(요청)**  \n",
    "  - 각 인프라 결합 파일에서 `EMD_NM == '화곡동'` 필터 후 저장:  \n",
    "    - 공원녹지_화곡동.csv — rows=27,562  \n",
    "    - 교육인프라_화곡동.csv — rows=13,782  \n",
    "    - 유통시설_화곡동.csv — rows=13,782\n",
    "\n",
    "- **5. 마스터 결합(매매·전월세·표제부 + 인프라):**  \n",
    "  - 불러온 파일: `최종) 매매_전처리.csv`, `최종) 전월세_전처리.csv`, `03. 표제부_20260122144509.csv`  \n",
    "  - 전월세: `PNU`별 집계(갯수, 평균 보증금/월세, 최근 계약일) 생성(컬럼명 접두어로 충돌 회피).  \n",
    "  - 표제부: `새주소본번`/`새주소부번`(또는 `번`/`지`)로 `PNU` 생성 후 `PNU` 기준으로 주요 표제부 컬럼만 선택(중복 제거).  \n",
    "  - 인프라: 각 `*_combined.csv`에서 `PNU`별로 최소거리(`*_min_dist`)와 카운트(`*_count`) 집계.  \n",
    "  - 병합 순서: 매매(기준, left) ← 전월세요약 ← 표제부요약 ← 인프라집계 (left-join).  \n",
    "  - 결과 파일: master_combined.csv — rows=4,748, cols=41\n",
    "\n",
    "**컬럼 처리 및 중복 규칙 요약**\n",
    "\n",
    "- **PNU 처리:** 인프라에서 생성한 `PNU`를 기준으로 통합. 중복 `pnu` 컬럼이 있으면 삭제하고 `PNU`만 유지.  \n",
    "- **집계 컬럼 네이밍:** 전월세·인프라 집계는 접두사(`rent_`, `park_`, `edu_`, `dist_`)를 붙여 병합 시 이름 충돌을 피함.  \n",
    "- **표제부:** `PNU` 단위로 중복 제거(`drop_duplicates(subset=['PNU'])`) 후 필요한 열만 선택해 병합.  \n",
    "- **데이터 타입 정리:** 숫자형(예: `거래금액(만원)`, `보증금`)은 `,` 제거 후 `to_numeric`, 날짜는 `to_datetime(..., errors='coerce')` 적용.  \n",
    "- **남은 충돌:** 병합으로 `_x`/`_y` 접미사가 생기면 필요시 정리할 수 있음(현재는 의도적으로 최소화).\n",
    "\n",
    "**생성된 주요 파일(위치)**\n",
    "\n",
    "- 공원녹지_combined.csv  \n",
    "- 교육인프라_combined.csv  \n",
    "- 유통시설_combined.csv  \n",
    "- 공원녹지_화곡동.csv  \n",
    "- 교육인프라_화곡동.csv  \n",
    "- 유통시설_화곡동.csv  \n",
    "- master_combined.csv\n",
    "\n",
    "**권장되는 다음 단계 (선택)**\n",
    "\n",
    "- `_x/_y` 접미사 컬럼 정리 및 불필요 원본 컬럼 제거.  \n",
    "- master_combined.csv에 대한 변수 분포/결측 요약(예: `거래금액(만원)`, `보증금(만원)`, `건축연령` 등).  \n",
    "- VIF 제거 및 Stepwise(AIC/BIC/p-value) 기반 변수 선택 준비(모델용 전처리 스크립트 추가).  \n",
    "- 헤도닉 모델(적정가·잔차·매매-전세 시차) 구현 시작.\n",
    "\n",
    "원하시면 바로 (1) `_x/_y` 정리 및 마스터 컬럼 정리, (2) master_combined.csv의 변수별 요약(기술통계 + 히스토그램 샘플), 또는 (3) VIF 계산용 전처리(결측·스케일링) 중 어느 것을 먼저 할지 알려주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa8ffdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob, os, re\n",
    "\n",
    "BASE = '/Users/lopi/Downloads/pj'\n",
    "INFRA_FOLDERS = {\n",
    "    '공원녹지': '공원녹지',\n",
    "    '교육인프라': '교육인프라',\n",
    "    '유통시설': '유통시설',\n",
    "}\n",
    "COMMON_PNU_PREFIX = '11500103001'\n",
    "\n",
    "def load_and_concat(folder_name):\n",
    "    pattern = os.path.join(BASE, folder_name, '*.csv')\n",
    "    files = sorted(glob.glob(pattern))\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        # CSVs use '|' as separator in this project\n",
    "        df = pd.read_csv(f, sep='|', dtype=str, encoding='utf-8', low_memory=False)\n",
    "        df['__source_file'] = os.path.basename(f)\n",
    "        dfs.append(df)\n",
    "    if dfs:\n",
    "        return pd.concat(dfs, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def parse_hono(h):\n",
    "    if pd.isna(h):\n",
    "        return (None, None)\n",
    "    s = str(h).strip()\n",
    "    # match patterns like '1163-4' or '1164'\n",
    "    m = re.match(r'^(\\d+)(?:-(\\d+))?$', s)\n",
    "    if m:\n",
    "        bon = m.group(1)\n",
    "        bub = m.group(2) or '0'\n",
    "        return (bon, bub)\n",
    "    # fallback: extract first two numeric groups if present\n",
    "    parts = re.findall(r'\\d+', s)\n",
    "    if len(parts) == 0:\n",
    "        return (None, None)\n",
    "    if len(parts) == 1:\n",
    "        return (parts[0], '0')\n",
    "    return (parts[0], parts[1])\n",
    "\n",
    "# 로드 및 파싱 실행 (메모리 상황에 유의)\n",
    "infra_dfs = {}\n",
    "for label, folder in INFRA_FOLDERS.items():\n",
    "    print(f'Loading {label} from {folder}...')\n",
    "    df = load_and_concat(folder)\n",
    "    if df.empty:\n",
    "        print(f'  No files found for {label}')\n",
    "        infra_dfs[label] = df\n",
    "        continue\n",
    "    # ensure HONO_NM exists and parse it\n",
    "    if 'HONO_NM' not in df.columns:\n",
    "        print(f\"  Warning: 'HONO_NM' not in columns for {label}\")\n",
    "    df['HONO_NM'] = df.get('HONO_NM', pd.NA)\n",
    "    parsed = df['HONO_NM'].apply(parse_hono)\n",
    "    df['bonbun'] = parsed.apply(lambda x: x[0] if x else None)\n",
    "    df['bubun'] = parsed.apply(lambda x: x[1] if x else None)\n",
    "    df['bonbun4'] = df['bonbun'].fillna('0').astype(str).str.zfill(4)\n",
    "    df['bubun4'] = df['bubun'].fillna('0').astype(str).str.zfill(4)\n",
    "    df['pnu'] = COMMON_PNU_PREFIX + df['bonbun4'] + df['bubun4']\n",
    "    df['PNU'] = df['pnu']\n",
    "    infra_dfs[label] = df\n",
    "    out_path = os.path.join(BASE, f'{label}_combined.csv')\n",
    "    print(f'  saving combined -> {out_path} (rows={len(df)})')\n",
    "    df.to_csv(out_path, index=False, sep='|')\n",
    "\n",
    "# 노트북에서 이어서 확인할 수 있도록 infra_dfs 변수 유지\n",
    "infra_dfs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3916b106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 통합 데이터의 상위 몇 줄을 확인\n",
    "for k, df in infra_dfs.items():\n",
    "    print('---', k, '---')\n",
    "    display(df.head(3))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
